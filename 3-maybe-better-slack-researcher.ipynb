{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a200bd",
   "metadata": {},
   "source": [
    "# 3-better-slack-researcher\n",
    "> A collaborator has hundreds of Slack messages that have citations to research papers.\n",
    "\n",
    "Let's see if we can't incrementally improve on the naive approach.\n",
    "\n",
    "We need to do a few pip installs to get the environment setup\n",
    "\n",
    "```bash\n",
    "python3 -m venv crewai-venv\n",
    "source crewai-venv/bin/activate\n",
    "pip install ipykernel \"crewai[tools]\" python-dotenv langchain-community pymupdf pypdf2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5055d37",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from crewai import Agent, Crew, Task, Process\n",
    "from crewai_tools import WebsiteSearchTool\n",
    "from pydantic import Field, BaseModel\n",
    "from crewai.tools import BaseTool\n",
    "import requests\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77667df",
   "metadata": {},
   "source": [
    "## Creating our first Crew AI Crew\n",
    "\n",
    "Let's make a Crew whose purpose is to extract text from an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf73d2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Instantiate the tools\n",
    "We'll use the [WebsiteSearchTool](https://docs.crewai.com/tools/websitesearchtool) to navigate and extract information from a website based on semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4449ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VisionTool\n",
    "website_search_tool = WebsiteSearchTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d0fd3",
   "metadata": {},
   "source": [
    "### Create a Custom PDFDownloaderTool\n",
    "What if we...really did want to download the PDF? Well, turns out we can make our own custom tools.\n",
    "\n",
    "See more here: [Custom Tools](https://docs.crewai.com/how-to/create-custom-tools)\n",
    "\n",
    "You could also do this directly using the PyPDFLoader from langchain, but you would need to do some additional work to download the file. Crew.ai has integrations with the ridiculously incredible corpus of both [LangChain tools](https://docs.crewai.com/concepts/langchain-tools) and [LlamaIndex tools](https://docs.crewai.com/concepts/llamaindex-tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4246ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFDownloaderTool(BaseTool):\n",
    "    name: str = \"PDFDownloader\"\n",
    "    description: str = \"Useful for downloading PDF files from a given URL and returning the path and content\"\n",
    "\n",
    "    def _run(self, uri: str) -> str:\n",
    "        \"\"\"Download the pdf file from the given URL and return the path to the downloaded pdf file\"\"\"\n",
    "        try:\n",
    "            response = requests.get(uri)\n",
    "\n",
    "            # Get the end part of the uri as the file name\n",
    "            file_name = uri.split('/')[-1]\n",
    "\n",
    "            # if the file name already has a .pdf extension, use it otherwise add it\n",
    "            file_path = file_name if file_name.endswith('.pdf') else file_name + '.pdf'\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                \n",
    "                # Use PyPDFLoader to extract text from the downloaded PDF\n",
    "                pdf_loader = PyPDFLoader(file_path=file_path,\n",
    "                                         extract_images=True,\n",
    "                                         mode=\"single\")\n",
    "                text_content = pdf_loader.load()\n",
    "                \n",
    "                return {\n",
    "                    \"file_path\": file_path,\n",
    "                    \"content\": text_content\n",
    "                }\n",
    "            else:\n",
    "                return f\"Failed to download file: {response.status_code}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error downloading PDF: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0b374",
   "metadata": {},
   "source": [
    "## Formatting the output\n",
    "We can also used structured outputs on the output format if we want to have a more structured result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b754f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForumListOutput(BaseModel):\n",
    "    title: str = Field(description=\"The title of the forum post\")\n",
    "    file_path: str = Field(description=\"The path to the downloaded PDF file\")\n",
    "    abstract: str = Field(description=\"The abstract of the paper\")\n",
    "    summary: str = Field(description=\"A paragraph summarizing the most important points of the paper to an expert in the field\")\n",
    "    link: str = Field(description=\"The URL link to the pdf file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d5e0d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Define the Agent\n",
    "Visit https://docs.crewai.com/concepts/agents#direct-code-definition to learn more. Below are the full options for the agent.\n",
    "This basically resolves to determining the role of the agent, specifically:\n",
    "* The role title\n",
    "* The goal of the agent\n",
    "* The backstory of the agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "779bf52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with all available parameters\n",
    "archivist_agent = Agent(\n",
    "    role=\"Modern Research Archivist\",\n",
    "    goal=\"Find and download the the latest research articles, blogs, and papers based on the user's query\",\n",
    "    backstory=\"You are an experienced research archivist who meticulously collects and organizes research papers, articles, and other online media. \"\n",
    "              \"You are an expert in using Arxiv, extracting technical writing from blogs and websites, and tracking down references made on twitter and other social media platforms.\",\n",
    "    llm=\"gpt-4o-mini\",  # Default: OPENAI_MODEL_NAME or \"gpt-4\"\n",
    "    function_calling_llm=None,  # Optional: Separate LLM for tool calling\n",
    "    verbose=True,  # *******Default: False\n",
    "    tools=[website_search_tool, PDFDownloaderTool()],  # *******Optional: List of tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eca900",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Define the task\n",
    "Learn more here: [Task](https://docs.crewai.com/concepts/tasks)\n",
    "\n",
    "The purpose of a task is to define the assignment that an agent will complete. Minimally, you need a description of the task to be completed, the expected output, and the agent that is responsible to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "616ea67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define the task\n",
    "main_task = Task(\n",
    "    description=\"Find and download the research articles, blog, paper, or other online technical writing based on the information at the following website URL: {provided_link}\",\n",
    "    expected_output=\"A paragraph summarizing the most important points of the paper to an expert in the field and a link to the paper\",\n",
    "    agent=archivist_agent,\n",
    ")\n",
    "\n",
    "#%% Define a formatting task to separate the output formatting from the main task\n",
    "formatting_task = Task(\n",
    "    description=\"Format the output of the main task based on a pydantic schema to report the results of the fetch, summarization, and download\",\n",
    "    expected_output=\"An object with the following fields: title, file_path, abstract, summary, link\",\n",
    "    agent=archivist_agent,\n",
    "    output_pydantic=ForumListOutput #******* Note that we have added this pydantic output to the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f391d9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Define the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a3d0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crew\n",
    "crew = Crew(\n",
    "    agents=[archivist_agent],\n",
    "    tasks=[main_task, formatting_task],\n",
    "    process = Process.sequential,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e941dab",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Run the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986746e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the crew\n",
    "result = crew.kickoff({\"provided_link\": \"https://arxiv.org/pdf/2501.13533\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13a486",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3cd59f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Print the result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "crewai-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
